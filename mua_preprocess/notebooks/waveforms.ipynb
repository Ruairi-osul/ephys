{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "kilosort_folder = r'C:\\Users\\Rory\\raw_data\\CIT_WAY\\dat_files\\cat'\n",
    "recording = r'2018-05-01_01'\n",
    "sep = '\\\\'\n",
    "\n",
    "num_spikes_for_averaging = 1000\n",
    "num_channels = 32\n",
    "num_samples_per_waveform = 120\n",
    "temp_folder =r'C:\\Users\\Rory\\Documents\\Work\\temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [''.join(['Chan_', str(num)]) for num in range(1, 33)]\n",
    "path = os.path.join(kilosort_folder, recording, recording) + '.dat'\n",
    "waveform_window = np.arange(-num_samples_per_waveform/2,\n",
    "                                 num_samples_per_waveform/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kilosort_arrays(recording):\n",
    "    '''\n",
    "    Loads arrays generated during kilosort into numpy arrays and pandas DataFrames\n",
    "    Parameters:\n",
    "        recording       = name of the recording being analysed\n",
    "    Returns:\n",
    "        spike_clusters  = numpy array of len(num_spikes) identifying the cluster from which each spike arrose\n",
    "        spike_times     = numpy array of len(num_spikes) identifying the time in samples at which each spike occured\n",
    "        cluster_groups  = pandas DataDrame with one row per cluster and column 'cluster_group' identifying whether\n",
    "                          that cluster had been marked as 'Noise', 'MUA' or 'Good'\n",
    "    '''\n",
    "    spike_clusters = np.load('spike_clusters.npy')\n",
    "    spike_times = np.load('spike_times.npy')\n",
    "    cluster_groups = pd.read_csv('cluster_groups.csv', sep='\\t')\n",
    "    try:  # check data quality\n",
    "        assert np.shape(spike_times.flatten()) == np.shape(spike_clusters)\n",
    "    except AssertionError:\n",
    "        AssertionError('Array lengths do not match in recording {}'.format(\n",
    "            recording))\n",
    "    return spike_clusters, spike_times, cluster_groups\n",
    "\n",
    "\n",
    "def load_data(recording, kilosort_folder, verbose, sep):\n",
    "    '''\n",
    "    Loads arrays generated during kilosort into numpy arrays and pandas DataFrames\n",
    "    Parameters:\n",
    "        recording       = name of the recording being analysed\n",
    "        kilosort_folder = the name of the root directory in which subdirectories for each recording are stored\n",
    "                          inside the sub-directories should be the files generated during spike sorting with\n",
    "                          kilosort and phy\n",
    "        verbose         = True or False\n",
    "        sep             = os directory delimeter e.g. '/'\n",
    "    Returns:\n",
    "        spike_clusters  = numpy array of len(num_spikes) identifying the cluster from which each spike arrose\n",
    "        spike_times     = numpy array of len(num_spikes) identifying the time in samples at which each spike occured\n",
    "        cluster_groups  = pandas DataDrame with one row per cluster and column\n",
    "                          'cluster_group' identifying whetherthat cluster had been marked as 'Noise', 'MUA' or 'Good'\n",
    "    '''\n",
    "    if verbose:\n",
    "        print('\\nLoading Data:\\t{}\\n'.format(recording))\n",
    "    os.chdir(sep.join([kilosort_folder, recording]))\n",
    "    spike_clusters, spike_times, cluster_groups = load_kilosort_arrays(\n",
    "        recording)\n",
    "    return spike_clusters, spike_times, cluster_groups\n",
    "\n",
    "\n",
    "def get_good_cluster_numbers(cluster_groups_df):\n",
    "    '''\n",
    "    Takes the cluster_groups pandas DataFrame fomed during data loading and returns a numpy array of cluster\n",
    "    ids defined as 'Good' during kilosort and phy spike sorting\n",
    "    Parameters:\n",
    "        cluster_groups_df   = the pandas DataFrame containing information on which cluster is 'Good', 'Noise' etc.\n",
    "    Returns:\n",
    "        A numpy array of 'Good' cluster ids\n",
    "    '''\n",
    "    good_clusters_df = cluster_groups_df.loc[cluster_groups_df['group'] == 'good', :]\n",
    "    return good_clusters_df['cluster_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_good_spikes_df(good_cluster_numbers, spike_clusters, spike_times):\n",
    "    df = pd.DataFrame({'spike_time':  spike_times.flatten(), 'spike_cluster':spike_clusters.flatten()})\n",
    "    df = df.loc[df['spike_cluster'].isin(good_cluster_numbers), :]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spiketimes_series(df, cluster, num_spikes):\n",
    "    spike_times = df.loc[df['spike_cluster']==cluster, 'spike_time'].iloc[:num_spikes]\n",
    "    spike_times.index = range(len(spike_times))  # change index \n",
    "    return spike_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_data(path):\n",
    "    temp = np.memmap(path, dtype=np.int16)\n",
    "    total_len = len(temp)\n",
    "    real_len = int(total_len/num_channels)\n",
    "    raw_data = np.memmap(path, dtype=np.int16, shape=(real_len, num_channels))\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveforms_from_raw_data(num_spikes_for_averaging, num_samples_per_waveform, \n",
    "                                num_channels, spike_times, waveform_window,\n",
    "                               raw_data):\n",
    "    \n",
    "    empty_template = np.zeros((num_spikes_for_averaging,\n",
    "                               num_samples_per_waveform,\n",
    "                               num_channels))\n",
    "    \n",
    "    for spike in range(num_spikes_for_averaging):\n",
    "        start_index = int(spike_times.iloc[spike]+waveform_window[0])  # start of waveform in raw data\n",
    "        end_index = int((spike_times.iloc[spike]+waveform_window[-1])+1)  # end of waveform in raw data\n",
    "\n",
    "        waveform = raw_data[start_index:end_index, 0:num_channels]  # extract waveform from raw data\n",
    "        empty_template[spike, :, :] = waveform[:,:]  #  add extracted waveform to 3d matrix\n",
    "    mean_waveform = np.mean(empty_template, axis=0)\n",
    "    waveform_per_channel_df = pd.DataFrame(mean_waveform, columns=cols)\n",
    "    return waveform_per_channel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_max_amp_channel(df):\n",
    "    maxes = df.apply(np.max, axis=0)\n",
    "    correct_chan = df.loc[:, maxes.idxmax()]\n",
    "    return correct_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_for_peak_finding(chan_df):\n",
    "    ave_waveform = pd.DataFrame({'y_values':chan_df})\n",
    "    ave_waveform['reverse_change'] = ave_waveform.diff(periods=-1)\n",
    "    ave_waveform['diff_reverse'] = np.where(ave_waveform.y_values.diff(periods=-1) > 0, 'increase', 'degrease')\n",
    "    return ave_waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_down_up(df, cluster, thresh=0.1):\n",
    "    \n",
    "    peak_amp = df['y_values'].max()\n",
    "    peak_sample = df['y_values'].idxmax()\n",
    "    \n",
    "    min_amp = df['y_values'].min()\n",
    "    min_sample = df['y_values'].idxmin()\n",
    "    \n",
    "    baseline_amp = peak_amp * thresh\n",
    "    baseline_sample = df.loc[(df['y_values']<peak_amp) \n",
    "                       & (df.index < peak_sample) \n",
    "                       & (df['diff_reverse']=='decline') \n",
    "                       & (df['y_values']>=baseline_amp), 'y_values'].idxmin()\n",
    "    \n",
    "    SW_peak = np.absolute(peak_sample - baseline_sample)/30\n",
    "    SW_troff = np.absolute(min_sample - baseline_sample)/30\n",
    "    SW_base = np.absolute(return_baseline_sample - baseline_sample)/30\n",
    "    \n",
    "    clu_df = pd.DataFrame({'cluster': str(cluster), 'SW_peak': SW_peak, 'SW_troff': SW_troff, 'SW_base': SW_base})\n",
    "    \n",
    "    return clu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Rory\\\\raw_data\\\\CIT_WAY\\\\dat_files\\\\cat\\\\2018-05-01_01\\\\2018-05-01_01.dat'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data =  load_raw_data(path)\n",
    "spike_clusters, spike_times, cluster_groups = load_data(recording=recording,\n",
    "                                                        kilosort_folder=kilosort_folder,\n",
    "                                                        verbose=False,\n",
    "                                                        sep=sep)\n",
    "good_cluster_numbers = get_good_cluster_numbers(cluster_groups)\n",
    "df = create_good_spikes_df(good_cluster_numbers, spike_clusters, spike_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_waveforms(recording, good_spikes_df, cluster, num_spikes,\n",
    "                         num_samples_per_waveform, num_channels, waveform_window,\n",
    "                         raw_data, temp_folder):\n",
    "    \n",
    "    \n",
    "    spike_times = create_spiketimes_series(df, \n",
    "                                           cluster=cluster, \n",
    "                                           num_spikes=num_spikes)\n",
    "    \n",
    "    df_all_chans = get_waveforms_from_raw_data(num_spikes_for_averaging, num_samples_per_waveform, \n",
    "                                num_channels, spike_times, waveform_window,\n",
    "                               raw_data)\n",
    "    df_max_chan = choose_max_amp_channel(df_all_chans)\n",
    "    ave_waveform = create_df_for_peak_finding(chan_df=df_max_chan)\n",
    "    \n",
    "    f, a = plt.subplots(ncols=2, figsize=(12,8))\n",
    "    df_all_chans.plot(ax=a[0])\n",
    "    df_max_chan.plot(ax=a[1])\n",
    "    \n",
    "    plt.savefig(os.path.join(temp_folder, recording) + str(cluster) + '.png')\n",
    "    plt.close()\n",
    "    try:\n",
    "        clu_df = up_down_up(df=ave_waveform, cluster=cluster_to_plot, thresh=0.1)\n",
    "    except:\n",
    "        with open(os.path.join(temp_folder, 'bad_waves.txt'), mode='a') as file:\n",
    "            file.write('Bad Waveform in {rec}: Cluster {clu}\\n'.format(rec=recording, clu=cluster))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in good_cluster_numbers:\n",
    "    get_cluster_waveforms(recording=recording,\n",
    "                          good_spikes_df=df, \n",
    "                          cluster=cluster, \n",
    "                          num_spikes=num_spikes_for_averaging,\n",
    "                          num_samples_per_waveform=num_samples_per_waveform,\n",
    "                          num_channels=num_channels,\n",
    "                          waveform_window=waveform_window,\n",
    "                          raw_data=raw_data, \n",
    "                          temp_folder=temp_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "    - Work around spikes with no positive peak\n",
    "    - Turn into scirpt (loop over recordings, loop over clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

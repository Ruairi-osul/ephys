{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Kilosort Data to MYSQL DB\n",
    "\n",
    "#### We'll use sqlalchemy and pandas to connect to the db\n",
    "    - database username and password saved as envirnment variables\n",
    "    - These can be found and added at ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sql\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "db_user = os.environ.get('DB_USER')\n",
    "db_pass = os.environ.get('DB_PASS')\n",
    "\n",
    "db = 'mua_data'\n",
    "\n",
    "con_str =f\"mysql+pymysql://{db_user}:{db_pass}@localhost/{db}\"\n",
    "eng = sql.create_engine(con_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our function for adding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_db(path, con_str, tbl, mode='append', v=True):\n",
    "    if v:\n",
    "        print('Adding {}'.format(os.path.basename(path)))\n",
    "    \n",
    "    eng = sql.create_engine(con_str)\n",
    "    df = pd.read_csv(path)\n",
    "    df.to_sql(name=tbl,\n",
    "              index=False,\n",
    "              if_exists=mode,\n",
    "              con=eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Non Kilosort data if not already present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding experiments.csv\n",
      "Adding eeg_configs.csv\n",
      "Adding eeg_chans.csv\n",
      "Adding probes.csv\n",
      "Adding probe_chans.csv\n",
      "Adding experimental_groups.csv\n",
      "Adding mice.csv\n",
      "Adding recordings.csv\n"
     ]
    }
   ],
   "source": [
    "# dir of data entered manually\n",
    "ff_dir = '/home/ruairi/data/db'\n",
    "tbls_todo = ['experiments', 'eeg_configs', 'eeg_chans',\n",
    "        'probes', 'probe_chans', 'experimental_groups',\n",
    "         'mice', 'recordings']\n",
    "\n",
    "for f in tbls_todo:\n",
    "    path = os.path.join(ff_dir, ''.join([f + '.csv']))\n",
    "    add_to_db(path=path, con_str=con_str, tbl=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add kilosort data: Neurons, Waveforms, Spiketimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create recording objects for each recording to do\n",
    "\n",
    "##### a) the Recording class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recording:\n",
    "    \n",
    "    def __init__(self, exp, dd_dir, r_id, **kwargs):\n",
    "        self.exp = exp\n",
    "        self.dd_dir = dd_dir\n",
    "        self.id = r_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) intantiate the objects and store then in the list: todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ruairi/repos/ephys/scripts')\n",
    "import preprocess as pp\n",
    "\n",
    "\n",
    "e_query = 'SELECT experiment_name, dat_file_dir FROM experiments;'\n",
    "r_qry = 'SELECT recording_id, dat_filename FROM recordings;'\n",
    "\n",
    "experiments = pd.read_sql(e_query, eng, index_col='experiment_name')\n",
    "recordings = pd.read_sql(r_qry, eng)\n",
    "\n",
    "\n",
    "todo = []\n",
    "for i, exp_name in enumerate(experiments.index):\n",
    "    parent = experiments.iloc[i]['dat_file_dir']\n",
    "    for recording in pp.get_subfolders(parent):\n",
    "        base = os.path.basename(recording)\n",
    "        r_id = recordings[recordings['dat_filename']==base]['recording_id'].values[0]\n",
    "        \n",
    "        todo.append(Recording(exp=exp_name,\n",
    "                             dd_dir=recording,\n",
    "                             r_id=r_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get spikes and waveforms for each recording and write to db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_data(recording):\n",
    "    spike_times = pp.get_spike_times(p=recording.dd_dir, r_id=recording.id)\n",
    "    waveforms, chans = pp.get_waveforms(spike_times, rd=recording.dd_dir)\n",
    "    neurons = spike_times.drop('spike_times', axis=1).drop_duplicates()\n",
    "    neurons = pd.merge(left=neurons, right=chans[['cluster_id', 'channel']], on='cluster_id')\n",
    "    return spike_times, waveforms, chans, neurons\n",
    "\n",
    "    \n",
    "def update_db(spike_times, waveforms, chans, neurons, eng):\n",
    "    # write neurons to db then retreive again for neuron_id keys\n",
    "    n_qry= f'SELECT neuron_id, cluster_id FROM neurons WHERE recording_id={recording.id}'\n",
    "    neurons.to_sql('neurons', eng, if_exists='append', index=False)\n",
    "    neuron_ids = pd.read_sql(n_qry, eng)\n",
    "    \n",
    "    # merge spike_times and waveforms with the new neuron ids and format as in db\n",
    "    waveforms = pd.merge(right=neuron_ids[['neuron_id', 'cluster_id']],\n",
    "                         left=waveforms, on='cluster_id').drop('cluster_id', axis=1)\n",
    "    waveforms = waveforms[['neuron_id', 'sample', 'value']]\n",
    "    spike_times = pd.merge(left=neuron_ids[['neuron_id', 'cluster_id']],\n",
    "                         right=spike_times, on='cluster_id').drop(['cluster_id', 'recording_id'],\n",
    "                                                               axis=1)\n",
    "    \n",
    "    # write to db\n",
    "    waveforms.drop_duplicates().to_sql('waveform_timepoints', eng, if_exists='append', index=False)\n",
    "    spike_times.drop_duplicates().to_sql('spike_times', eng, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "updating\n",
      "2\n",
      "updating\n",
      "3\n",
      "updating\n",
      "4\n",
      "updating\n",
      "8\n",
      "updating\n",
      "10\n",
      "updating\n",
      "11\n",
      "updating\n",
      "12\n",
      "updating\n",
      "13\n",
      "updating\n",
      "14\n",
      "updating\n",
      "15\n",
      "updating\n",
      "16\n",
      "updating\n",
      "17\n",
      "updating\n",
      "19\n",
      "updating\n",
      "22\n",
      "updating\n",
      "21\n",
      "updating\n",
      "23\n",
      "updating\n",
      "27\n",
      "updating\n",
      "18\n",
      "updating\n",
      "25\n",
      "updating\n",
      "20\n",
      "updating\n",
      "26\n",
      "updating\n",
      "24\n",
      "updating\n",
      "5\n",
      "updating\n",
      "6\n",
      "updating\n",
      "7\n",
      "updating\n",
      "9\n",
      "updating\n",
      "28\n",
      "updating\n",
      "29\n",
      "updating\n",
      "30\n",
      "updating\n",
      "31\n",
      "updating\n",
      "32\n",
      "updating\n",
      "33\n",
      "updating\n",
      "34\n",
      "updating\n",
      "35\n",
      "updating\n",
      "36\n",
      "updating\n",
      "38\n",
      "updating\n",
      "39\n",
      "updating\n",
      "40\n",
      "updating\n",
      "41\n",
      "updating\n",
      "42\n",
      "updating\n",
      "43\n",
      "updating\n",
      "44\n",
      "updating\n"
     ]
    }
   ],
   "source": [
    "didnt_work = []\n",
    "for recording in todo:\n",
    "    print(recording.id)\n",
    "    spike_times, waveforms, chans, neurons = get_new_data(recording)\n",
    "    \n",
    "    try:\n",
    "        print('updating')\n",
    "        update_db(spike_times, waveforms, chans, neurons, eng)\n",
    "    except ValueError:\n",
    "        didnt_work.append(recording.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
